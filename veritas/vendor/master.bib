@inproceedings{karimi-metaease-nsdi-2026,
  author      = {Pantea Karimi and
                Siva Kakarla and
                Pooria Namyar  and
                Santiago Segarra and
                Ryan Beckett  and
                Mohammad Alizadeh and
                Behnaz Arzani},
  title       = {MetaEase: Heuristic Analysis from Source Code via Symbolic-Guided Optimization},
  booktitle   = {Proceedings of the 23rd {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI})},
  series      = {NSDI '26},
  publisher   = {{USENIX} Association},
  year        = {2026},
  abstract    = {COMING SOON},
  code        = {https://github.com/microsoft/MetaEase}
}

@inproceedings{hussien-coresync-model-mascots-2025,
  author      = {Jehad Hussien and
                Pratyush Sahu and
                Eric Stuhr and
                Ahmed Saeed},
  title       = {Modeling the Interactions between Core Allocation and Overload Control in μs-Scale Network Stacks},
  booktitle   = {33rd International Symposium on the Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS) Workshop},
  series      = {MASCOTS '25},
  publisher   = {IEEE},
  year        = {2025},
  url         = {http://saeed.github.io/files/coresync_model-mascots25.pdf},
  code        = {https://github.com/GT-ANSR-Lab/core-allocation_overload-control_model},
  slides      = {http://saeed.github.io/files/coresync_model_mascots25_ppt.pdf},
  abstract  = {Modern datacenter operators aim to maximize the utilization of limited and expensive resources, especially CPU cores. Achieving such an objective requires fast and accurate core scheduling policies. Meanwhile, operating at high utilization requires the employment of overload controllers that shed excess load beyond the allocated capacity. Currently, no analytical techniques exist to study the interactions between these controllers. In this paper, we use performance verification to establish bounds on the throughput and latency achieved by a server that employs state-of-the-art fine-grained core allocation and overload control mechanisms. Our model enables system performance analysis under a wide range of workload and system configurations (e.g., RTT, load, and burstiness). We show that worst-case throughput and latency degrade by 1.8× and 2.3×, respectively, under high load and burstiness due to the interactions between overload control and fast core allocation. We validate our findings using simulations that demonstrate the plausibility of the identified worst-case behavior under realistic conditions.}
}

@inproceedings{bortha-archie-hotnets-2024,
  author       = {Rahul Bothra and
                  Venkat Arun and
                  Brighten Godfrey and
                  Akshay Narayan and
                  Ahmed Saeed},
  title        = {Lightweight Automated Reasoning for Network Architectures},
  booktitle    = {Proceedings of the 23rd {ACM} Workshop on Hot Topics in Networks (HotNets)},
  series       = {HotNets '24},
  pages        = {237--245},
  publisher    = {{ACM}},
  year         = {2024},
  url          = {http://saeed.github.io/files/archie_hoetnetst24.pdf},
  doi          = {10.1145/3696348.3696865},
  abstract = {Architecting a modern data center network is increasingly complicated. Seeking the highest performance and support for emerging workloads, network architects planning a build-out must choose from a large selection of switching components, NICs, network stacks, congestion control algorithms, routing schemes, measurement systems, virtualization software, centralized bandwidth allocators and security mechanisms, all from various vendors. Today, manual planning by human experts is time-consuming at best, and can easily result in overlooked design choices or missed complex inter-dependencies. We propose a radical departure from typical whiteboard-and-spreadsheet planning, and ask: is it possible to reason automatically about possible network architectural designs? Such an approach is nontrivial since formal reasoning about even a single component (like routing systems) is difficult, and we seek to understand how a variety of functional components fit together. We explore the challenge through examples and propose an automated lightweight reasoning framework that models architectural complexities at a broad, but shallow, level of abstraction. Such a framework could serve as a useful design tool for network architects, for careful cross-team planning, and even to help vendors plan product features and requirements.}
}


@techreport{virelay-goel-tr-2023,
  author      = {Saksham Goel and
                  Benjamin Mikek and
                  Jehad Hussien and
                  Venkat Arun and
                  Ahmed Saeed and
                  Aditya Akella},
  title       = {A Performance Verification Methodology for Resource Allocation Heuristics},
  booktitle    = {arXiv},
  year        = {2023},
  url          = {https://arxiv.org/pdf/2301.04205},
  doi          = {10.48550/ARXIV.2301.04205},
  code        = {https://github.com/kazikame/quant-ver/},
  abstract    = {Performance verification is a nascent but promising tool for understanding the performance and limitations of heuristics under realistic assumptions. Bespoke performance verification tools have already demonstrated their value in settings like congestion control and packet scheduling. In this paper, we aim to emphasize the broad applicability and utility of performance verification. To that end, we highlight the design principles of performance verification. Then, we leverage that understanding to develop a set of easy-to-follow guidelines that are applicable to a wide range of resource allocation heuristics. In particular, we introduce Virelay, a framework that enables heuristic designers to express the behavior of their algorithms and their assumptions about the system in an environment that resembles a discrete-event simulator. We demonstrate the utility and ease-of-use of Virelay by applying it to six diverse case studies. We produce bounds on the performance of classical algorithms, work stealing and SRPT scheduling, under practical assumptions. We demonstrate Virelay's expressiveness by capturing existing models for congestion control and packet scheduling, and we verify the observation that TCP unfairness can cause some ML training workloads to spontaneously converge to a state of high network utilization. Finally, we use Virelay to identify two bugs in the Linux CFS load balancer.}
}

@phdthesis{arun2023verifying,
  title={Verifying the Performance of Network Control Algorithms},
  author={Arun, Venkat},
  year={2023},
  school={Massachusetts Institute of Technology}
}


@inproceedings{arun-starvation-sigcomm-2023,
  author    = {Venkat Arun and
               Mohammad Alizadeh and
               Hari Balakrishnan},
  title     = {Starvation in end-to-end congestion control},
  booktitle = {Proceedings of the 2023 Annual Conference of the {ACM} Special Interest Group on Data Communication ({SIGCOMM})},
  series    = {SIGCOMM '23},
  publisher = {{ACM}},
  year      = {2023},
  url       = {https://dl.acm.org/doi/pdf/10.1145/3544216.3544223},
  abstract  = {To overcome weaknesses in traditional loss-based congestion control algorithms (CCAs), researchers have developed and deployed several delay-bounding CCAs that achieve high utilization without bloating delays (e.g., Vegas, FAST, BBR, PCC, Copa, etc.). When run on a path with a fixed bottleneck rate, these CCAs converge to a small delay range in equilibrium. This paper proves a surprising result: although designed to achieve reasonable inter-flow fairness, current methods to develop delay-bounding CCAs cannot always avoid starvation, an extreme form of unfairness. Starvation may occur when such a CCA runs on paths where non-congestive network delay variations due to real-world factors such as ACK aggregation and end-host scheduling exceed double the delay range that the CCA converges to in equilibrium. We provide experimental evidence for this result for BBR, PCC Vivace, and Copa with a link emulator. We discuss the implications of this result and posit that to guarantee no starvation an efficient delay-bounding CCA should design for a certain amount of non-congestive jitter and ensure that its equilibrium delay oscillations are at least one-half of this jitter.}}



@inproceedings{arun-ccac-sigcomm-2021,
  author    = {Venkat Arun and
               Mina Tahmasbi Arashloo and
               Ahmed Saeed and
               Mohammad Alizadeh and
               Hari Balakrishnan},
  title     = {Toward Formally Verifying Congestion Control Behavior},
  booktitle = {Proceedings of the 2021 Annual Conference of the {ACM} Special Interest Group on Data Communication ({SIGCOMM})},
  series    = {SIGCOMM '21},
  publisher = {{ACM}},
  year      = {2021},
  doi       = {10.1145/3452296.3472912},
  pages  = {1--16},
  url         = {http://saeed.github.io/files/ccac-sigcomm21.pdf},
  code      = {https://github.com/venkatarun95/ccac},
  video     = {https://www.youtube.com/watch?v=D_BjxRsXvC4},
  gadget   = {https://projects.csail.mit.edu/ccac/},
  abstract = {The diversity of paths on the Internet makes it difficult for designers and operators to confidently deploy new congestion control algorithms (CCAs) without extensive real-world experiments, but such capabilities are not available to most of the networking community. And even when they are available, understanding why a CCA under-performs by trawling through massive amounts of statistical data from network connections is challenging. The history of congestion control is replete with many examples of surprising and unanticipated behaviors unseen in simulation but observed on real-world paths. In this paper, we propose initial steps toward modeling and improving our confidence in a CCA's behavior. We have developed Congestion Control Anxiety Controller (CCAC), a tool that uses formal verification to establish certain properties of CCAs. It is able to prove hypotheses about CCAs or generate counterexamples for invalid hypotheses. With CCAC, a designer can not only gain greater confidence prior to deployment to avoid unpleasant surprises, but can also use the counterexamples to iteratively improve their algorithm. We have modeled additive-increase/multiplicative-decrease (AIMD), Copa, and BBR with CCAC, and describe some surprising results from the exercise.}
}

